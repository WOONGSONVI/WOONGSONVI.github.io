---
layout: single
title: "[PyTorch로 시작하는 딥러닝 입문] 3장 머신러닝 입문하기 (1)_선형 회귀"
categories: [PyTorch]
tag: [PyTorch, Machine Learning, Deep Learning]
use_math: true
author_profile: false
typora-root-url: ../
---
-----
안녕하세요. 23년 하반기는 정말 바쁜 시기였습니다. 글도 제대로 못 쓸 정도로요... 조만간에 어떤 일들이 있었는지, 앞으로 무엇을 할 건지 등 일상글도 작성해보도록 하겠습니다 ㅎㅎㅎ 마지막 12월에는 파이토치를 이용한 선형 회귀에 대해 소개해드리겠습니다!

## 3-1. 선형 회귀(Linear Regression)

1.&nbsp;<u><b>가설(Hypothesis) 수립</b></u>

선형 회귀식은 아래와 같이 가정해보겠습니다.

$y=H(x)=Wx+b$

참고로 가설의 $H$를 따서 $y$를 $H(x)$라고도 표현합니다. $W$와 $b$는 각각 가중치와 편향을 뜻합니다.

<br>

2.&nbsp;<u><b>비용 함수(Cost function)에 대한 이해</b></u>

딥러닝을 공부하면서 헷갈렸던 부분인데 <mark style='background-color: #f39393'>비용 함수(Cost function), 손실 함수(Loss function), 오차 함수(Error function), 목적 함수(Objective function)</mark>는 모두 같은 용어를 뜻합니다. 비용 함수에 대해서 알아보기 전에 오차에 대한 개념을 먼저 알아야 합니다. <mark style='background-color: #7ff5a0'>오차</mark>는 '실제값 - 예측값'을 뜻합니다. 오차제곱합을 수식으로 나타내면 아래와 같습니다.

> $\sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2$

오차제곱합을 데이터의 개수인 $n$으로 나눈 값을 <mark style='background-color: #7ff5a0'>평균 제곱 오차(Mean Squared Error ; MSE)</mark>라고 부릅니다.

> $\frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2$

선형 회귀의 목적은 이러한 MSE를 최솟값으로 만드는 $W$와 $b$를 찾아내는 것입니다. 즉, 아래의 값을 최소로 만드는 초모수 값을 찾아 내는 것이 비용 함수의 목적입니다.

> $cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2$

<br>

3.&nbsp;<u><b>최적화(Optimizer) - 경사 하강법(Gradient Descent)</b></u>

그렇다면 비용 함수의 값을 최소로 만드는 초모수를 찾는 방법에 대해서 알아봅시다. 일반적으로 최적화(Optimizer) 알고리즘이 사용되는데 그 중 가장 기본적인 방법인 <mark style='background-color: #7ff5a0'>경사 하강법</mark>에 대해 알아보겠습니다. 예를 들어, $H(x)=Wx$에서 $W$와 비용의 관계를 그래프로 표현하면 아래와 같습니다.

![img](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG){: .align-center}

머신러닝 모델은 가장 최소의 비용값을 찾아내기 위해 점점 접선의 기울기가 0이 되도록 $W$를 수정하는 것을 볼 수 있습니다. 즉, $W$ 값은 아래와 같은 수식으로 반복적으로 조정됩니다.

> $W := W - α\frac{∂}{∂W}cost(W)$

여기서 <mark style='background-color: #7ff5a0'>$α$</mark>는 학습률(Learning Rate)을 뜻하며, $W$ 값을 변경할 때, 얼마나 크게 변경할지를 결정합니다. 너무 학습률을 크게 설정하면 접선의 기울기가 $0$이 되는 $W$를 찾는 것이 아니라 비용값을 발산시키게 됩니다. 또한, 너무 작게 설정하면 학습 속도가 느려지므로, 적정한 $α$을 찾아내야 합니다.

![img](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG){: .align-center}

<br>

4.&nbsp;<u><b>파이토치로 선형 회귀 구현하기</b></u>

- (1) 기본 셋팅


```python
import torch      # 메인 네임스페이스
import torch.nn as nn     # 신경망을 구축하기 위한 다양한 데이터 구조나 레이어
import torch.nn.functional as F     # 함수
import torch.optim as optim     # 파라미터 최적화 알고리즘 구현

torch.manual_seed(1)      # 랜덤 시드 설정
```

- (2) 변수 선언

X_train과 y_train 모두 (3, 1) 텐서인 것 확인

```python
X_train = torch.FloatTensor([[1], [2], [3]])      # X_train 데이터 생성
y_train = torch.FloatTensor([[4], [5], [6]])      # y_train 데이터 생성

print(X_train)
print('---------------')
print(X_train.shape)
print('---------------')
print(y_train)
print('---------------')
print(y_train.shape)
--------------------------------------------------------------------------------------------------------------------------------
tensor([[1.],
        [2.],
        [3.]])
---------------
torch.Size([3, 1])
---------------
tensor([[4.],
        [5.],
        [6.]])
---------------
torch.Size([3, 1])
```

- (3) 가중치와 편향의 초기화

선형 회귀의 목적은 비용 함수를 최소로 만드는 $W, b$를 찾는 것입니다. 우선은 $W, b$를 0으로 초기화합니다. 그리고 <mark style='background-color: #f39393'>requires_grad = True</mark>로 인자를 줍니다. 이는 학습을 통해 값이 변경되는 변수임을 뜻하며, 앞으로 이 텐서에 대한 기울기를 저장한다는 것을 의미합니다.

```python
W = torch.zeros(1, requires_grad = True)      # 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.
b = torch.zeros(1, requires_grad = True)      # 가중치 b를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.

print('W :', W)     # 가중치 W 출력
print('b :', b)     # 가중치 b 출력
--------------------------------------------------------------------------------------------------------------------------------
W : tensor([0.], requires_grad=True)
b : tensor([0.], requires_grad=True)
```

즉, 현재 비용함수는 아래와 같습니다.

>$y = 0 × x + 0$

- (4) 가설 세우기

$H(x)= Wx+b$에 대한 가설을 선언합니다.

```python
hypothesis = X_train * W + b
print(hypothesis)
--------------------------------------------------------------------------------------------------------------------------------
tensor([[0.],
        [0.],
        [0.]], grad_fn=<AddBackward0>)
```

- (5) 비용 함수 선언하기

선형 회귀의 비용 함수인 $MSE = cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2$를 선언합니다.

```python
cost = torch.mean((hypothesis - y_train) ** 2)
print(cost)
--------------------------------------------------------------------------------------------------------------------------------
tensor(25.6667, grad_fn=<MeanBackward0>)
```

- (6) 경사 하강법 구현하기

일반적인 경사하강법(Gradient Descent)은 하나 이상의 국소최솟값(Local Minimum)이 존재하거나 안장점(Saddle Point)이 존재할 때, 비용함수를 최소로 만드는 초모수 값을 못 찾을 가능성이 있습니다. 따라서, 이에 대한 대안으로 확률적 경사 하강법인 <mark style='background-color: #7ff5a0'>SGD(Stochastic Gradient Descent)</mark>를 사용합니다.

조금 더 자세히 설명드리겠습니다. 손실함수 H에 대하여 $h_{i}$를 $i$번째 표본에 대한 손실이라고 할 때, $n$개의 데이터에 대한 총 손실은 $H(W, b) = \sum_{i=1}^{n} h_{i}$가 됩니다. SGD는 총 손실에 대한 최적화가 아닌 {h_{1}, h_{2}, \cdots ,h_{n}}을 차례로 하나씩 최소화하여 궁극적으로 전체 손실함수 $H(W, b)$를 최소화합니다. 이 방법은 표본의 임의성을 이용해 최적화 시, 국소최솟값이나 안정점에 빠지는 위험을 줄여줍니다. 다만, 수렴방향이 너무 임의로 움직이는 현상인 <mark style='background-color: #f39393'>잡음(Noise)</mark>가 발생할 수 있습니다. 따라서, 하나하나의 표본으로 하지 않고, 전체 표본 $n$을 $k$개의 <mark style='background-color: #f39393'>미니 배치</mark>로 나눠서 사용합니다. 전체 표본에 대해 한 바퀴를 돌아 모수의 최신화가 이루어지면 <mark style='background-color: #f39393'>1 에폭(epoch)</mark>이 완성되었다고 합니다.

여기서부터 코딩 작성 업데이트

<br>

4.&nbsp;<u><b>#</b></u>

#

```python
#
```

```python
#
--------------------------------------------------------------------------------------------------------------------------------
#
```

<br>

5.&nbsp;<u><b>#</b></u>

#

```python
#	
```

```python
#
--------------------------------------------------------------------------------------------------------------------------------
#
```

#

```python
#
```

```python
#
--------------------------------------------------------------------------------------------------------------------------------
#
```

#

```python
#
```

```python
#
--------------------------------------------------------------------------------------------------------------------------------
#
```

<br>

6.&nbsp;<u><b>#</b></u>

#

```python
#
```

```python
#
--------------------------------------------------------------------------------------------------------------------------------
#
```

```python
#
--------------------------------------------------------------------------------------------------------------------------------
#
```

<br>

#### *Reference*

1. <mark style='background-color: #0550ae'><b><a href='https://wikidocs.net/book/2788'><font color="white">Pytorch로 시작하는 딥러닝 입문</font></a></b></mark>

<br>

<img src="https://user-images.githubusercontent.com/37182279/216820587-4617a62e-0565-47f1-9ead-f4cd367572a1.png" alt="DATA_100%_LOGO_LIGHT" style="zoom:10%">{: .align-center}

<br>

<br>



